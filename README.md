================================================================================
                STAR: Spatio-Temporal Action Recognition
================================================================================

ðŸš€ End-to-end transformer-based model for detecting actions in video clips 
with spatial localization using DETR-inspired architecture and R(2+1)D backbone.

[Model Weights ðŸ¤—]

================================================================================

Python: 3.8+ | PyTorch | License: MIT | Model: 40M params


KEY FEATURES
================================================================================

âœ¨ End-to-End Detection - No external proposals or NMS required
ðŸŽ¯ DETR-Style Architecture - Set-based prediction with Hungarian matching
ðŸŽ¬ Spatio-Temporal Understanding - 3D CNN backbone + Transformer decoder
âš¡ Mixed Precision Training - Efficient training with AMP
ðŸ“¦ Lightweight - 155MB model, runs on CPU or GPU
ðŸ”§ Easy Deployment - Simple inference API with bounding box visualization
ðŸŽ¯ Generic Action Detection - Adaptable to any action categories


WHAT IT DOES
================================================================================

This model performs spatio-temporal action detection in videos:
    â€¢ Identifies WHAT action is occurring (classification)
    â€¢ Localizes WHERE in the frame (bounding box)
    â€¢ Processes video clips end-to-end
    â€¢ Outputs predictions with confidence scores
    â€¢ Generates annotated videos with visualizations

Use cases: Action recognition, event detection, video understanding, 
automated video analysis, behavior monitoring


MODEL WEIGHTS
================================================================================

Pre-trained weights available on Hugging Face:

ðŸ¤— Download: https://huggingface.co/spaces/ayushsaun/video-action-transformer

Or use the Hugging Face Hub:
    from huggingface_hub import hf_hub_download
    model_path = hf_hub_download(repo_id="YOUR_USERNAME/video-action-transformer", 
                                  filename="best_model.pth")


PROJECT STRUCTURE
================================================================================

video-action-transformer/
â”œâ”€â”€ main.py                Training script
â”œâ”€â”€ test.py                Inference script with visualization
â”œâ”€â”€ requirements.txt       Python dependencies
â”œâ”€â”€ Report.pdf             Report of work done in the pipeline
â””â”€â”€ README.md             This file

Note: Model weights (best_model.pth) hosted separately on Hugging Face


REQUIREMENTS
================================================================================

- Python 3.8 or higher
- CUDA-capable GPU (recommended, but CPU works)
- 4GB+ GPU memory for training
- 2GB+ GPU memory for inference
- 1GB disk space for model weights


INSTALLATION
================================================================================

Quick Setup (3 commands):

1. Clone repository:
   git clone https://github.com/ayushsaun24024/video-action-transformer.git
   cd video-action-transformer

2. Install dependencies:
   pip install -r requirements.txt

3. Download model weights:
   wget https://huggingface.co/spaces/ayushsaun/video-action-transformer/resolve/main/best_model.pth

Optional: Create virtual environment
   python3 -m venv star_env
   source star_env/bin/activate  # Linux/Mac
   star_env\Scripts\activate     # Windows


QUICK START
================================================================================

Inference (Detect actions in your videos):

python3 test.py \
  --input_dir ./test_videos \
  --output_file predictions.csv \
  --model_path best_model.pth

Output:
  â€¢ predictions.csv - Detection results with bounding boxes
  â€¢ annotated_videos/ - Videos with bounding boxes drawn
  â€¢ test_log_*.txt - Execution log


Training (Train on your custom dataset):

python3 main.py --input_dir /path/to/dataset --output_dir ./output

See DATASET STRUCTURE section below for data format requirements.


DATASET STRUCTURE
================================================================================

Training Dataset (for main.py)
-------------------------------

Prepare your dataset in this structure:

dataset/
â”œâ”€â”€ action_challenge.csv
â”œâ”€â”€ video1.mp4
â”œâ”€â”€ video2.mp4
â”œâ”€â”€ label1.txt
â””â”€â”€ label2.txt

CSV Format (action_challenge.csv):
    video_file,label_file
    video1.mp4,label1.txt
    video2.mp4,label2.txt
    video3.mp4,

Label File Format (.txt):
    <class_id> <x1> <y1> <x2> <y2>
    
    class_id: Integer class ID (e.g., 0, 1, 2, ...)
    x1, y1, x2, y2: Bounding box coordinates in pixels (xyxy format)

Example (label1.txt):
    0 150 200 450 600

Note: Class IDs are automatically detected and mapped during training.
Configure your action classes in Config.CLASS_NAMES in main.py


Testing Dataset (for test.py)
------------------------------

For inference, you only need video files:

test_videos/
â”œâ”€â”€ video1.mp4
â”œâ”€â”€ video2.avi
â””â”€â”€ video3.mov

Supported formats: .mp4, .avi, .mov, .mkv


USAGE
================================================================================

TRAINING (main.py)
------------------

Train the model on your custom action detection dataset:

python3 main.py --input_dir /path/to/dataset [--output_dir ./output]

Arguments:
    --input_dir    Dataset directory with videos, labels, CSV (required)
    --output_dir   Output directory (default: ./output)

Expected Files in input_dir:
    â€¢ action_challenge.csv (must be present)
    â€¢ Video files referenced in CSV
    â€¢ Label files referenced in CSV

Training Outputs (saved to output_dir):
    â€¢ split.json - Train/validation split information
    â€¢ best_model.pth - Best model checkpoint
    â€¢ training_results.csv - Training metrics per epoch
    â€¢ test_predictions.csv - Final validation predictions

Training Configuration:
    Epochs: 50 | Batch Size: 2 | Learning Rate: 5e-5
    Validation Split: 20% | Frames per Video: 16
    Image Size: 224Ã—224 | Optimizer: AdamW with Cosine Annealing

Note: Modify Config class in main.py to customize hyperparameters


TESTING/INFERENCE (test.py)
----------------------------

Run inference on video files and generate predictions:

python3 test.py \
  --input_dir /path/to/videos \
  --output_file predictions.csv \
  [--model_path best_model.pth] \
  [--score_threshold 0.3]

Arguments:
    --input_dir        Directory containing video files (required)
    --output_file      Output CSV file path (required)
    --model_path       Model checkpoint path (default: best_model.pth)
    --score_threshold  Detection confidence threshold (default: 0.3)

Outputs:
    1. predictions.csv
       Detection results with class predictions and bounding boxes
       Format: video_name,pred_class_1,x1_1,y1_1,x2_1,y2_1,pred_class_2,...
    
    2. annotated_videos/
       Videos with bounding boxes and labels drawn on frames
    
    3. test_log_YYYYMMDD_HHMMSS.txt
       Detailed execution log with timestamps

Bounding Box Format:
    Coordinates are in normalized xyxy format (0-1 range)
    Multiply by video width/height to get pixel coordinates

Example Output (predictions.csv):
    video_name,pred_class_1,x1_1,y1_1,x2_1,y2_1
    video1.mp4,action_0,0.234,0.456,0.678,0.890
    video2.mp4,no_object,0.0,0.0,0.0,0.0
    video3.mp4,action_1,0.345,0.567,0.789,0.901


MODEL ARCHITECTURE
================================================================================

Architecture Components:

Backbone: R(2+1)D-18
    â€¢ 3D CNN with factorized convolutions
    â€¢ Pretrained on Kinetics-400 dataset
    â€¢ Extracts spatio-temporal features

Encoder:
    â€¢ Learned positional embeddings
    â€¢ 1Ã—1 3D convolution for projection
    â€¢ Layer normalization + dropout

Decoder: Transformer Decoder
    â€¢ 4 layers with 8 attention heads
    â€¢ 2048-dimensional feedforward network
    â€¢ Cross-attention between queries and video features

Prediction Heads:
    â€¢ Classification Head: Action class probabilities
    â€¢ Bounding Box Head: Normalized (cx, cy, w, h) coordinates

Object Queries:
    â€¢ 12 learnable query embeddings
    â€¢ Each query can predict one action instance

Training Strategy:
    â€¢ Loss: DETR-style (Classification + 5Ã—L1 + 2Ã—GIoU)
    â€¢ Matching: Hungarian algorithm for bipartite matching
    â€¢ Optimizer: AdamW with weight decay
    â€¢ Scheduler: Cosine annealing learning rate
    â€¢ Precision: Mixed precision (FP16/FP32) with AMP

Model Statistics:
    Total Parameters: 40,509,220
    Model Size: ~155 MB
    Hidden Dimension: 256
    Number of Queries: 12


HOW IT WORKS
================================================================================

Training Pipeline:
    1. Load video clips and annotations
    2. Sample 16 frames uniformly from each video
    3. Extract features using R(2+1)D backbone
    4. Process through transformer decoder
    5. Match predictions to ground truth using Hungarian algorithm
    6. Compute combined loss (classification + localization)
    7. Update model weights via backpropagation

Inference Pipeline:
    1. Load video and sample 16 frames
    2. Forward pass through the model
    3. Apply confidence threshold to filter predictions
    4. Convert normalized boxes to pixel coordinates
    5. Draw bounding boxes on frames
    6. Save annotated video and CSV results


ADVANCED USAGE
================================================================================

Batch Processing Multiple Directories:

for video_dir in dir1/ dir2/ dir3/; do
    python3 test.py --input_dir $video_dir --output_file ${video_dir}_results.csv
done

Custom Confidence Threshold:

python3 test.py --input_dir ./videos --output_file out.csv --score_threshold 0.5

Force CPU Inference:

CUDA_VISIBLE_DEVICES="" python3 test.py --input_dir ./videos --output_file out.csv

Modify Action Classes:

Edit main.py Config class:
    NUM_CLASSES = 3  # Number of action classes
    CLASS_NAMES = ["action_A", "action_B", "action_C", "no_object"]


TROUBLESHOOTING
================================================================================

Out of Memory Error:
    Solution: Edit main.py Config class and set BATCH_SIZE = 1

CUDA Not Available:
    Info: Model automatically detects and uses CPU
    Note: Inference will be slower but functional (~5-10s per video)

Video Read Errors:
    â€¢ Install additional codecs: sudo apt-get install libavcodec-extra
    â€¢ Verify video integrity: ffmpeg -i video.mp4 -f null -
    â€¢ Check supported formats: .mp4, .avi, .mov, .mkv

Label Format Errors:
    â€¢ Ensure exactly 5 space-separated values per line
    â€¢ Verify class IDs are integers
    â€¢ Check bounding box coordinates are within video dimensions

Model Loading Errors:
    â€¢ Verify best_model.pth exists in specified path
    â€¢ Check file size (~155 MB)
    â€¢ Re-download from Hugging Face if corrupted


TECHNICAL DETAILS
================================================================================

Based on State-of-the-Art Research:
    â€¢ DETR (Detection Transformer) - Carion et al., ECCV 2020
    â€¢ STAR (Spatio-Temporal Action Recognition) - Gritsenko et al., CVPR 2024
    â€¢ R(2+1)D Networks - Tran et al., CVPR 2018

Architecture Adaptations:
    âœ“ Simplified for practical deployment
    âœ“ Clip-level prediction (16 frames per video)
    âœ“ Single bounding box per action instance
    âœ“ Optimized for small to medium datasets
    âœ“ CPU-compatible for edge deployment

Key Design Choices:
    â€¢ Hungarian matching ensures one-to-one correspondence
    â€¢ GIoU loss improves localization quality
    â€¢ Mixed precision training speeds up convergence
    â€¢ Balanced batch sampling prevents class imbalance


CUSTOMIZATION
================================================================================

To adapt this model for your specific use case:

1. Modify Action Classes:
   Edit Config.CLASS_NAMES in main.py:
       CLASS_NAMES = ["your_action_1", "your_action_2", "no_object"]

2. Adjust Model Capacity:
   Edit Config in main.py:
       NUM_QUERIES = 20  # More queries for multiple instances
       HIDDEN_DIM = 512  # Larger model capacity

3. Change Training Duration:
   Edit Config in main.py:
       NUM_EPOCHS = 100  # Longer training
       LEARNING_RATE = 1e-4  # Different learning rate

4. Modify Evaluation Threshold:
   Pass argument to test.py:
       --score_threshold 0.7  # Higher confidence threshold


CITATION
================================================================================

If you use this code in your research or project, please cite:

@software{video_action_transformer_2026,
  author = {Ayush Saun and Aman Sharma},
  title = {STAR: Spatio-Temporal Action Recognition},
  year = {2026},
  url = {https://github.com/YOUR_USERNAME/video-action-transformer}
}

Original DETR paper:

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and 
          Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}


LICENSE
================================================================================

MIT License - See LICENSE file for details

This implementation is for educational and research purposes.


CONTRIBUTING
================================================================================

Contributions are welcome! Areas for improvement:
    â€¢ Multi-instance detection (multiple actions per video)
    â€¢ Temporal localization (per-frame predictions)
    â€¢ Additional action categories
    â€¢ Data augmentation strategies
    â€¢ Model compression (quantization, pruning)
    â€¢ Real-time inference optimization

Please open an issue or submit a pull request.


ACKNOWLEDGMENTS
================================================================================

â€¢ Meta AI Research for DETR architecture
â€¢ Google Research for STAR and R(2+1)D contributions
â€¢ PyTorch team for deep learning framework
â€¢ Hugging Face for model hosting platform
â€¢ Open-source computer vision community


SUPPORT
================================================================================

Issues: https://github.com/YOUR_USERNAME/video-action-transformer/issues
Model: https://huggingface.co/YOUR_USERNAME/video-action-transformer

For questions or bug reports:
    â€¢ Check log files for detailed error messages
    â€¢ Verify dataset format matches documentation
    â€¢ Ensure model weights are correctly downloaded
    â€¢ Review troubleshooting section above


================================================================================
                        End of Documentation
================================================================================
